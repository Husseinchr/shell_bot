
"""Malware Detection Module

Detects dangerous/malicious Linux commands using:
- Pattern matching for known dangerous commands
- Semantic analysis using WordNet and NLP pipeline
- POS and dependency analysis to detect destructive actions"""

import re
from typing import List, Set, Tuple
from nltk.corpus import wordnet as wn
from .nlp_pipeline import NLPPipeline


class MalwareDetector:
    """Detects malware and dangerous commands in natural language prompts."""
    
    def __init__(self):
        """Sets up the malware detector."""
        self.nlp_pipeline = NLPPipeline()
        

        self.dangerous_patterns = [
            r'Rm\s+-rf\s+/',
            r'Rm\s+-rf\s+/\*',
            r'Rm\s+-rf\s+~',
            r'Rm\s+-rf\s+\.\.',
            r'Format\s+',
            r'Dd\s+if=/dev/zero',
            r':\(\)\{:\|:\&\};:',
            r'Mkfs\.',
            r'Fdisk\s+/dev/',
            r'Chmod\s+-R\s+777\s+/',
            r'Chown\s+-R\s+.*\s+/',
            r'>\s+/dev/sd[a-z]',
            r'Sudo\s+rm\s+-rf\s+/',
            r'Sudo\s+.*\s+rm\s+-rf',
        ]
        

        self.system_dirs = [
            '/etc', '/sys', '/proc', '/dev', '/boot', '/root',
            '/usr/bin', '/usr/sbin', '/bin', '/sbin', '/lib',
            '/var/log', '/var/lib', '/opt', '/mnt', '/media'
        ]
        

        self.destructive_verbs = {
            'Delete', 'Remove', 'Erase', 'Destroy', 'Format', 'Wipe',
            'Clear', 'Purge', 'Eliminate', 'Obliterate', 'Annihilate',
            'Truncate', 'Overwrite', 'Corrupt', 'Damage', 'Break'
        }
        

        self.dangerous_intent_words = {
            'System', 'Root', 'All files', 'Everything', 'Entire',
            'Format disk', 'Wipe disk', 'Erase disk', 'Delete system',
            'Remove system', 'Destroy system', 'Corrupt system'
        }
        

        self._load_destructive_synsets()
    
    def _load_destructive_synsets(self):
        """Loads WordNet synsets for destructive actions."""
        self.destructive_synsets = set()
        for verb in self.destructive_verbs:
            synsets = wn.synsets(verb, pos=wn.VERB)
            for syn in synsets:
                self.destructive_synsets.add(syn)

                for hypernym in syn.hypernyms():
                    self.destructive_synsets.add(hypernym)
    
    def is_malware(self, text: str) -> bool:
        """Checks whether the input text contains malware or dangerous commands.
        Takes in:
        text: Input text (natural language or command)
        Gives back:
        true if malware detected, False otherwise"""
        if not text or not text.strip():
            return False
        
        text_lower = text.lower().strip()
        

        if self._check_dangerous_patterns(text_lower):
            return True
        


        if self._looks_like_command(text_lower):
            return self._check_command_danger(text_lower)
        

        if self._check_semantic_danger(text):
            return True
        

        if self._check_dependency_danger(text):
            return True
        
        return False
    
    def _check_dangerous_patterns(self, text: str) -> bool:
        """Check for dangerous command patterns."""
        for pattern in self.dangerous_patterns:
            if re.search(pattern, text, re.IGNORECASE):
                return True
        return False
    
    def _looks_like_command(self, text: str) -> bool:
        """Heuristic to detect if text is a direct command vs natural language.
        commands typically:
        - Start with a command name (no articles)
        - Contain flags like -r, -f, --option
         - Have paths like /path/to/file"""

        command_indicators = [
            r'^[a-z]+(\s+[-/][a-z]+)*',
            r'/[a-z/]+',
            r'\./[a-z]',
            r'\$[a-z_]+',
            r'&&|\|\||;',
        ]
        
        matches = sum(1 for pattern in command_indicators if re.search(pattern, text))
        return matches >= 2
    
    def _check_command_danger(self, command: str) -> bool:
        """Checks whether a direct command is dangerous."""

        if self._check_dangerous_patterns(command):
            return True
        

        for sys_dir in self.system_dirs:
            if sys_dir in command:

                destructive_ops = ['Rm', 'Delete', 'Remove', 'Format', 'Wipe', 'Erase']
                if any(op in command for op in destructive_ops):
                    return True
        

        if 'Sudo' in command:
            destructive_ops = ['Rm', 'Delete', 'Remove', 'Format', 'Mkfs', 'Fdisk', 'Dd']
            if any(op in command for op in destructive_ops):

                safe_patterns = ['Apt', 'Systemctl', 'Service', 'Update', 'Upgrade']
                if not any(pattern in command for pattern in safe_patterns):
                    return True
        
        return False
    
    def _check_semantic_danger(self, text: str) -> bool:
        """Use semantic analysis to detect dangerous intent."""

        result = self.nlp_pipeline.process(text)
        

        keywords = self.nlp_pipeline.get_keywords(text, filter_stopwords=True)
        

        text_lower = text.lower()
        for intent in self.dangerous_intent_words:
            if intent in text_lower:

                for verb in self.destructive_verbs:
                    if verb in text_lower:
                        return True
        

        destructive_found = False
        system_target_found = False
        
        for keyword in keywords:
            if keyword in self.destructive_verbs:
                destructive_found = True
            if any(sys_dir.replace('/', '') in keyword for sys_dir in self.system_dirs):
                system_target_found = True
        

        for lemma_info in result['Lemmas']:
            lemma = lemma_info['Lemma'].lower()
            if lemma in self.destructive_verbs:
                destructive_found = True
            else:

                for destructive_verb in self.destructive_verbs:
                    similarity = self.nlp_pipeline.semantic_similarity(lemma, destructive_verb)
                    if similarity > 0.7:
                        destructive_found = True
                        break
        

        for sys_dir in self.system_dirs:
            if sys_dir in text_lower or sys_dir.replace('/', ' ') in text_lower:
                system_target_found = True
        

        safe_verbs = {'List', 'Show', 'Display', 'Find', 'Search', 'Locate', 'View', 'See', 'Read', 'Print', 'Go', 'Navigate', 'Change', 'Enter', 'Cd', 'Copy', 'Cp', 'Move', 'Mv'}
        has_safe_verb = any(verb in text_lower for verb in safe_verbs)
        


        if destructive_found and system_target_found and not has_safe_verb:
            return True
        


        
        if destructive_found and not has_safe_verb:
            universal_targets = ['All', 'Everything', 'Entire', 'Whole', 'Complete']
            if any(target in text_lower for target in universal_targets):
                return True
        
        return False
    
    def _check_dependency_danger(self, text: str) -> bool:
        """Use dependency parsing to detect dangerous action-object relationships."""
        result = self.nlp_pipeline.process(text)
        doc = result['Doc']
        
        if doc is None:
            return False
        

        for token in doc:

            lemma = token.lemma_.lower()
            if lemma in self.destructive_verbs:

                for child in token.children:
                    if child.dep_ in ['Dobj', 'Pobj', 'Nmod']:
                        obj_text = child.text.lower()
                        

                        for sys_dir in self.system_dirs:
                            if sys_dir in obj_text or sys_dir.replace('/', ' ') in obj_text:
                                return True
                        

                        if any(word in obj_text for word in ['All', 'Everything', 'Entire']):
                            return True
        

        for chunk in result['Chunks']:
            chunk_lower = chunk.lower()

            for sys_dir in self.system_dirs:
                if sys_dir in chunk_lower:

                    for token in doc:
                        if token.lemma_.lower() in self.destructive_verbs:
                            return True
        
        return False
    
    def get_danger_reason(self, text: str) -> str:
        """Get the reason why a command is considered dangerous.
        Takes in:
        text: Input text
        Gives back:
        string describing the danger"""
        if not self.is_malware(text):
            return ""
        
        text_lower = text.lower()
        

        for pattern in self.dangerous_patterns:
            if re.search(pattern, text_lower):
                return f"Contains dangerous pattern: {pattern}"
        

        if self._check_semantic_danger(text):
            return "Contains destructive intent targeting system files/directories"
        

        if self._check_dependency_danger(text):
            return "Dependency analysis detected destructive action on system target"
        
        return "Malware detected"


if __name__ == "__main__":

    detector = MalwareDetector()
    
    test_cases = [
        ("I want to list all files in this directory", False),
        ("Delete all files in /etc directory", True),
        ("Remove the system files", True),
        ("rm -rf /", True),
        ("Format the disk", True),
        ("I want to copy a file to temp directory", False),
        ("Delete everything in the root directory", True),
        ("sudo rm -rf /var/log", True),
        ("List files in /etc", False),
        ("I want to enter the directory data then list the files", False),
    ]
    
    print("Testing Malware Detector")
    print("=" * 60)
    
    for text, expected in test_cases:
        result = detector.is_malware(text)
        status = "✓" if result == expected else "✗"
        print(f"{status} '{text}'")
        print(f"   Expected: {expected}, Got: {result}")
        if result:
            print(f"   Reason: {detector.get_danger_reason(text)}")
        print()

